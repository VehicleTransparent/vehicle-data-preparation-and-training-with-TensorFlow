{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1300b99e",
   "metadata": {},
   "source": [
    "# Vehicle Detection from images, videos, and Real-time detection with our trained model of custom dataSet with TensorFlow 2 Detection Model Zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1c2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc4dc3",
   "metadata": {},
   "source": [
    "# 1. Load Train Model From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957db114",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobilenet_v2_fpnlite_320x320' \n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
    "CHECKPOINT_NAME = 'ckpt-51'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f63297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44800aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2813a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'],CHECKPOINT_NAME )).expect_partial()\n",
    "\n",
    "# Set the minimum detection score threshold\n",
    "min_score_thresh = 0.6\n",
    "def detect_fn(image):\n",
    "    '''\n",
    "    takes an image in tensor type and return detections\n",
    "    '''\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e142322",
   "metadata": {},
   "source": [
    "# Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78cb0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehicle_detection_data_frame(image, detect_fn):\n",
    "    # convert image to tf tensor \n",
    "    input_tensor = tf.convert_to_tensor(image[np.newaxis, ...], dtype=tf.float32)\n",
    "    \n",
    "    # apply detect_fn on the input tensor\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # extract required information from detections\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    \n",
    "    # convert class ids to int\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    # filter detections with scores above threshold and convert bounding boxes to integer values\n",
    "    scores = detections['detection_scores']\n",
    "    boxes = detections['detection_boxes']\n",
    "    classes = detections['detection_classes']\n",
    "    num_detections = detections['num_detections']\n",
    "\n",
    "    my_detections = []\n",
    "    for i in range(num_detections):\n",
    "        if scores[i] >= min_score_thresh:\n",
    "            bbox = [int(a * b) for a, b in zip(boxes[i], image.shape[:2] * 2)]\n",
    "            detection = {\n",
    "                'score': scores[i],\n",
    "                'class': classes[i],\n",
    "                'box': bbox,\n",
    "            }\n",
    "            y1, x1, y2, x2 = bbox\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2) # \n",
    "            my_detections.append(detection)\n",
    "    \n",
    "    # return dataframe with detections or empty dataframe\n",
    "    print(my_detections)\n",
    "    if my_detections:\n",
    "        df = pd.DataFrame(my_detections)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['score', 'class', 'box'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584da85",
   "metadata": {},
   "source": [
    "## 2. Detect from an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b82bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.path.join(paths['IMAGE_PATH'], 'test', 'b1d9e136-9ab25cb3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3acebb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.95691293, 'class': 0, 'box': [274, 470, 511, 751]}, {'score': 0.8801923, 'class': 0, 'box': [292, 0, 520, 322]}, {'score': 0.7778156, 'class': 0, 'box': [297, 361, 390, 475]}, {'score': 0.7642996, 'class': 0, 'box': [227, 876, 656, 1280]}]\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(IMAGE_PATH)\n",
    "# img  = cv2.resize(img,(900,600))\n",
    "\n",
    "df = vehicle_detection_data_frame(image=img,detect_fn=detect_fn)\n",
    "\n",
    "# display image with bounding boxes\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612a6a2",
   "metadata": {},
   "source": [
    "## 3. Detect from a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5054425",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.8725246, 'class': 0, 'box': [659, 817, 920, 1126]}, {'score': 0.67323154, 'class': 0, 'box': [673, 1702, 1063, 1914]}]\n",
      "[{'score': 0.80368334, 'class': 0, 'box': [653, 816, 921, 1129]}, {'score': 0.7018361, 'class': 0, 'box': [675, 1693, 1065, 1913]}]\n",
      "[{'score': 0.84197813, 'class': 0, 'box': [653, 812, 923, 1123]}, {'score': 0.67077726, 'class': 0, 'box': [671, 1689, 1063, 1912]}]\n",
      "[{'score': 0.82004005, 'class': 0, 'box': [649, 811, 921, 1123]}, {'score': 0.63916874, 'class': 0, 'box': [637, 1680, 1071, 1916]}]\n",
      "[{'score': 0.83923256, 'class': 0, 'box': [656, 807, 923, 1130]}, {'score': 0.62725526, 'class': 0, 'box': [635, 1675, 1068, 1916]}]\n",
      "[{'score': 0.8326284, 'class': 0, 'box': [657, 811, 923, 1126]}, {'score': 0.65444887, 'class': 0, 'box': [613, 1673, 1068, 1916]}]\n",
      "[{'score': 0.7850558, 'class': 0, 'box': [646, 806, 916, 1123]}, {'score': 0.6532972, 'class': 0, 'box': [605, 1659, 1063, 1918]}]\n",
      "[{'score': 0.81275386, 'class': 0, 'box': [644, 806, 918, 1118]}, {'score': 0.6807338, 'class': 0, 'box': [594, 1649, 1064, 1920]}]\n",
      "[{'score': 0.8476327, 'class': 0, 'box': [637, 800, 920, 1119]}, {'score': 0.70892984, 'class': 0, 'box': [584, 1642, 1065, 1920]}]\n",
      "[{'score': 0.81966215, 'class': 0, 'box': [636, 803, 915, 1119]}, {'score': 0.7476342, 'class': 0, 'box': [593, 1631, 1063, 1920]}]\n",
      "[{'score': 0.83321726, 'class': 0, 'box': [635, 796, 914, 1120]}, {'score': 0.77222556, 'class': 0, 'box': [583, 1620, 1063, 1920]}]\n",
      "[{'score': 0.79549444, 'class': 0, 'box': [630, 795, 911, 1119]}, {'score': 0.77426773, 'class': 0, 'box': [588, 1619, 1063, 1920]}]\n",
      "[{'score': 0.8448433, 'class': 0, 'box': [627, 797, 912, 1119]}, {'score': 0.78828037, 'class': 0, 'box': [602, 1613, 1063, 1920]}]\n",
      "[{'score': 0.778502, 'class': 0, 'box': [585, 1609, 1065, 1920]}, {'score': 0.76868904, 'class': 0, 'box': [627, 792, 910, 1117]}]\n",
      "[{'score': 0.8305613, 'class': 0, 'box': [628, 791, 913, 1118]}, {'score': 0.80608577, 'class': 0, 'box': [586, 1604, 1066, 1920]}]\n",
      "[{'score': 0.80542725, 'class': 0, 'box': [629, 787, 915, 1121]}, {'score': 0.80006903, 'class': 0, 'box': [577, 1597, 1066, 1920]}]\n",
      "[{'score': 0.82991415, 'class': 0, 'box': [635, 795, 917, 1128]}, {'score': 0.811442, 'class': 0, 'box': [584, 1593, 1068, 1920]}]\n",
      "[{'score': 0.86140496, 'class': 0, 'box': [636, 794, 917, 1127]}, {'score': 0.8156448, 'class': 0, 'box': [585, 1586, 1068, 1920]}]\n",
      "[{'score': 0.86845875, 'class': 0, 'box': [637, 790, 917, 1128]}, {'score': 0.79602194, 'class': 0, 'box': [580, 1585, 1068, 1920]}]\n",
      "[{'score': 0.86599976, 'class': 0, 'box': [637, 788, 919, 1128]}, {'score': 0.8255938, 'class': 0, 'box': [572, 1576, 1064, 1920]}]\n",
      "[{'score': 0.8235057, 'class': 0, 'box': [577, 1570, 1067, 1920]}, {'score': 0.772279, 'class': 0, 'box': [657, 787, 922, 1130]}]\n",
      "[{'score': 0.8349468, 'class': 0, 'box': [653, 784, 927, 1132]}, {'score': 0.80533415, 'class': 0, 'box': [578, 1565, 1068, 1920]}]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('video11.mp4')\n",
    "# width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "    cv2.resize(frame,(800,600))\n",
    "    df = vehicle_detection_data_frame(image=frame,detect_fn=detect_fn)\n",
    "    cv2.imshow('object detection',  cv2.resize(frame, (800, 600)))\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae21ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf39",
   "language": "python",
   "name": "tf39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
